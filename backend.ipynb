{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d82340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn streamlit pyngrok pandas requests prophet tsfresh scikit-learn matplotlib    --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd685c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(\"38CzutWtzHRyqZxsMguNei4dsWn_4DLzAS36iKKgEuuf6F5FD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c9cf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend.py\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.concurrency import run_in_threadpool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "from prophet import Prophet\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "app = FastAPI(title=\"FitPulse â€“ Fast Backend\")\n",
    "\n",
    "# -------------------------------\n",
    "# CORS\n",
    "# -------------------------------\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "CLEAN_DF = None\n",
    "FEATURE_DF = None\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ERROR HANDLER\n",
    "# -------------------------------\n",
    "@app.exception_handler(Exception)\n",
    "async def global_exception_handler(request, exc):\n",
    "    trace = traceback.format_exc()\n",
    "    print(trace)\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"error\": \"Backend crashed\", \"trace\": trace}\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PREPROCESS (FAST)\n",
    "# -------------------------------\n",
    "@app.post(\"/preprocess\")\n",
    "async def preprocess(file: UploadFile = File(...)):\n",
    "    global CLEAN_DF\n",
    "\n",
    "    df = pd.read_csv(file.file)\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"date_time\": \"timestamp\",\n",
    "        \"date\": \"timestamp\",\n",
    "        \"heart_rate_avg\": \"heart_rate\",\n",
    "        \"step_count\": \"steps\",\n",
    "        \"total_steps\": \"steps\",\n",
    "        \"sleep_hours\": \"sleep\"\n",
    "    })\n",
    "\n",
    "    REQUIRED = [\"timestamp\", \"user_id\", \"heart_rate\", \"steps\", \"sleep\"]\n",
    "    for c in REQUIRED:\n",
    "        if c not in df.columns:\n",
    "            return JSONResponse(status_code=400, content={\"error\": f\"Missing {c}\"})\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    df[\"heart_rate\"] = pd.to_numeric(df[\"heart_rate\"], errors=\"coerce\").fillna(df[\"heart_rate\"].median())\n",
    "    df[\"steps\"] = pd.to_numeric(df[\"steps\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"sleep\"] = pd.to_numeric(df[\"sleep\"], errors=\"coerce\").fillna(df[\"sleep\"].median())\n",
    "\n",
    "    # Resample to daily average for consistency\n",
    "    df = (\n",
    "        df.set_index(\"timestamp\")\n",
    "        .groupby(\"user_id\")[[\"heart_rate\", \"steps\", \"sleep\"]]\n",
    "        .resample(\"D\")\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    CLEAN_DF = df\n",
    "    return {\"status\": \"success\", \"rows\": len(df)}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# FEATURE EXTRACTION (TSFresh)\n",
    "# -------------------------------\n",
    "def _feature_extraction_job():\n",
    "    global CLEAN_DF, FEATURE_DF\n",
    "\n",
    "    df = CLEAN_DF.copy()\n",
    "\n",
    "    # Fill NaNs before TSFresh\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # Use MinimalFCParameters for efficiency\n",
    "    settings = MinimalFCParameters()\n",
    "    \n",
    "    # Re-structure for TSFresh (long format)\n",
    "    df_long = df.melt(id_vars=[\"user_id\", \"timestamp\"], value_vars=[\"heart_rate\", \"steps\", \"sleep\"],\n",
    "                      var_name=\"kind\", value_name=\"value\")\n",
    "    \n",
    "    extracted_features = extract_features(\n",
    "        timeseries_container=df_long,\n",
    "        column_id=\"user_id\",\n",
    "        column_sort=\"timestamp\",\n",
    "        column_kind=\"kind\",\n",
    "        column_value=\"value\",\n",
    "        default_fc_parameters=settings\n",
    "    )\n",
    "    \n",
    "    # FEATURE_DF for Prophet needs to be the time series data\n",
    "    df_ts = df.sort_values([\"user_id\", \"timestamp\"]).copy()\n",
    "    \n",
    "    # Add manual features to time series for per-point analysis\n",
    "    df_ts[\"hr_7d_mean\"] = df_ts.groupby(\"user_id\")[\"heart_rate\"].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
    "    df_ts[\"hr_7d_std\"] = df_ts.groupby(\"user_id\")[\"heart_rate\"].transform(lambda x: x.rolling(7, min_periods=1).std().fillna(0))\n",
    "    \n",
    "    FEATURE_DF = df_ts\n",
    "    \n",
    "    return len(extracted_features)\n",
    "\n",
    "\n",
    "@app.post(\"/feature-extraction\")\n",
    "async def feature_extraction():\n",
    "    if CLEAN_DF is None:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Run /preprocess first\"})\n",
    "\n",
    "    num_features = await run_in_threadpool(_feature_extraction_job)\n",
    "\n",
    "    return {\"status\": \"success\", \"rows\": len(FEATURE_DF), \"tsfresh_users_processed\": num_features}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PROPHET (Seasonality + Anomalies)\n",
    "# -------------------------------\n",
    "def _prophet_job():\n",
    "    global FEATURE_DF\n",
    "    \n",
    "    results = {}\n",
    "    METRICS = [\"heart_rate\", \"steps\", \"sleep\"]\n",
    "\n",
    "    for metric in METRICS:\n",
    "        p_df = (\n",
    "            FEATURE_DF.groupby(\"timestamp\")[metric]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"timestamp\": \"ds\", metric: \"y\"})\n",
    "        )\n",
    "\n",
    "        if len(p_df) <= 10:\n",
    "            continue\n",
    "\n",
    "        model = Prophet()\n",
    "        model.fit(p_df)\n",
    "\n",
    "        # Forecast on history for anomalies\n",
    "        forecast = model.predict(p_df)\n",
    "        p_df[\"yhat\"] = forecast[\"yhat\"]\n",
    "        p_df[\"yhat_lower\"] = forecast[\"yhat_lower\"]\n",
    "        p_df[\"yhat_upper\"] = forecast[\"yhat_upper\"]\n",
    "        \n",
    "        p_df[\"anomaly\"] = (p_df[\"y\"] < p_df[\"yhat_lower\"]) | (p_df[\"y\"] > p_df[\"yhat_upper\"])\n",
    "        \n",
    "        # Future forecast\n",
    "        future = model.make_future_dataframe(periods=30, freq='D')\n",
    "        future_forecast = model.predict(future)\n",
    "        \n",
    "        # Combine\n",
    "        combined = pd.concat([p_df, future_forecast.iloc[len(p_df):]])\n",
    "        \n",
    "        # Select relevant columns\n",
    "        combined = combined[[\"ds\", \"y\", \"yhat\", \"yhat_lower\", \"yhat_upper\", \"anomaly\"]]\n",
    "        \n",
    "        combined[\"anomaly\"] = combined[\"anomaly\"].fillna(False)\n",
    "        combined[\"y\"] = combined[\"y\"].fillna(0)\n",
    "        combined = combined.fillna(0)\n",
    "        \n",
    "        results[metric] = combined.tail(60).to_dict(\"records\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@app.post(\"/prophet-forecast\")\n",
    "async def prophet_forecast():\n",
    "    if FEATURE_DF is None:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Run /feature-extraction first\"})\n",
    "\n",
    "    results = await run_in_threadpool(_prophet_job)\n",
    "\n",
    "    return {\"status\": \"success\", \"metrics\": results}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Clustering + Anomaly-Detection\n",
    "# -------------------------------\n",
    "\n",
    "def _cluster_anomaly_job():\n",
    "    \n",
    "    def dbscan_clustering(df):\n",
    "\n",
    "        user_features = (\n",
    "            df.groupby(\"user_id\")[[\"heart_rate\", \"steps\", \"sleep\"]]\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(user_features)\n",
    "\n",
    "        dbscan = DBSCAN(eps=1.2, min_samples=10)\n",
    "        clusters = dbscan.fit_predict(X)\n",
    "\n",
    "        user_features[\"cluster\"] = clusters\n",
    "\n",
    "        return user_features.reset_index()\n",
    "\n",
    "\n",
    "    def detect_cluster_anomalies(cluster_df):\n",
    "        \"\"\"\n",
    "        DBSCAN-based anomaly detection.\n",
    "        Noise points (-1) are anomalies.\n",
    "        \"\"\"\n",
    "\n",
    "        cluster_df[\"cluster_anomaly\"] = cluster_df[\"cluster\"] == -1\n",
    "        return cluster_df\n",
    "\n",
    "    # Step 1: clustering\n",
    "    cluster_df = dbscan_clustering(FEATURE_DF)\n",
    "\n",
    "    # Step 2: anomaly detection\n",
    "    cluster_df = detect_cluster_anomalies(cluster_df)\n",
    "\n",
    "    # Step 3: map back\n",
    "    FEATURE_DF[\"cluster\"] = FEATURE_DF[\"user_id\"].map(\n",
    "        cluster_df.set_index(\"user_id\")[\"cluster\"]\n",
    "    )\n",
    "\n",
    "    FEATURE_DF[\"cluster_anomaly\"] = FEATURE_DF[\"user_id\"].map(\n",
    "        cluster_df.set_index(\"user_id\")[\"cluster_anomaly\"])\n",
    "\n",
    "\n",
    "@app.post(\"/cluster-anomaly\")\n",
    "async def cluster_anomaly():\n",
    "    if FEATURE_DF is None:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Run /feature-extraction first\"})\n",
    "\n",
    "    results = await run_in_threadpool(_cluster_anomaly_job)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"summary\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ece57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI URL: NgrokTunnel: \"https://unbombastically-iracund-tiara.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [19036]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52573 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52573 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     2405:201:f:a9bf:6d32:7660:217b:8cd5:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     2405:201:f:a9bf:6d32:7660:217b:8cd5:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     2405:201:f:a9bf:6d32:7660:217b:8cd5:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     2405:201:f:a9bf:6d32:7660:217b:8cd5:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"FastAPI URL:\", public_url)\n",
    "\n",
    "threading.Thread(\n",
    "    target=uvicorn.run,\n",
    "    kwargs={\"app\": \"backend:app\", \"host\": \"0.0.0.0\", \"port\": 8000}\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2748b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
